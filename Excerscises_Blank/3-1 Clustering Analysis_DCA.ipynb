{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488c8082",
   "metadata": {},
   "source": [
    "<img src=\"../images/logo.png\" alt=\"slb\" style= \"width: 1700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# âš¡ï¸  - Tutorial 1\n",
    "\n",
    "## `PART 1`: Clustering Analysis\n",
    "\n",
    "ðŸ’¡ Clustering refers to grouping similar data points together, based on their attributes or features\n",
    "\n",
    "This tutorial will be divided into two parts:\n",
    "\n",
    "âœ” In the first part, we will explore various clustering methods using log data from a single well\n",
    "\n",
    "âœ” In the second part, we will use the production dataset to generate a model that predicts the production trend for one well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from numpy import nan as NA\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ Step 2: Import and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the 'w5.csv' file\n",
    "\n",
    "w5_logs= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics of the log data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ Step 3: Data Preprocessing -  Standardization\n",
    "\n",
    "ðŸ’¡ Some of the clustering models are distance based algorithms, in order to measure similarities between observations and form clusters they use a distance metric. So, features with high ranges will have a bigger influence on the clustering. \n",
    "\n",
    "âœðŸ» Therefore, it's a good practice to scale the data before applying clustering analysis.\n",
    "\n",
    "We will explore two methods to standardize the dataset: StandardScaler() and MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“‹ The **StandardScaler** is a method of standardizing data such the the transformed feature has a mean 0 and and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the StandardScaler() function to standardize each column of the w5_logs dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# First define the scaler\n",
    "\n",
    "\n",
    "# Transform the dataset using the scaler\n",
    "\n",
    "# w5_logs_scaled.describe().round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the MinMaxScaler() function to standardize each column of the w5_logs dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# First define the scaler\n",
    "\n",
    "\n",
    "# Transform the dataset using the scaler\n",
    "\n",
    "# w5_logs_scaled.describe().round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the standardized dataset and assign 'DEPTH' as index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ Step 4: Clustering Analysis Using K-Means\n",
    "\n",
    "ðŸ“‹ The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extract 3 clusters of data using K-Means. Use the raw data (Not Scaled Data)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "# Display the VpVs versus AI using the previously run K-means algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use the scaled data. Extract 3 cluster of data using K-Means. \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the VpVs versus AI using the previously run K-means algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ Step 5: Define a Plotting Function\n",
    "\n",
    "ðŸ’¥ As we will be exploring several clustering methods which result requires the same plot, it's a good idea to define a plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot clustering results\n",
    "\n",
    "def format_plot(title=\"\",pred=\"\"):\n",
    "    \n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ Step 6: Identify the Optimun Number of Clusters - Elbow Method ðŸ’ªðŸ¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ In this step, we are going to use *yellowbrick*, which is a python library typically used to QC different steps of a ML workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can use the elbow plot to decide about the best number of clusters\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the data to the visualizer\n",
    "\n",
    "\n",
    "\n",
    "# Draw the elbow plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“‹ **Distortion**: It is calculated as the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ðŸ’¡ Good solutions arenâ€™t those with the lowest score but rather the ones where you notice a more or less abrupt discontinuity in the descent of the score (even just a change in the slope). \n",
    "\n",
    "ðŸ’ªðŸ¼ In this example, a good solution seems to be four clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ Step 7: Clustering Analysis Using Gaussian Mixture Models (GMM)\n",
    "\n",
    "ðŸ’¡ Instead of using a distance-based model (K-means), we will now use a distribution-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster the log data using a Gaussian Mixture Model (GMM)\n",
    "\n",
    "# First, train the gaussian model \n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "\n",
    "# Generate predictions from the gaussian mixture model \n",
    "\n",
    "\n",
    "\n",
    "# Plot the result using the format_plot function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ Step 8: Clustering Analysis Using the Hierarchical Method\n",
    "\n",
    "ðŸ“‹ Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. \n",
    "\n",
    "The *AgglomerativeClustering* object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. \n",
    "\n",
    "\n",
    "ðŸ“ The linkage criteria determines the metric used for the merge strategy:\n",
    "\n",
    " - **Ward linkage** minimizes the sum of squared differences within all clusters\n",
    "\n",
    " - **Complete linkage** minimizes the maximum distance between observations of pairs of clusters\n",
    " \n",
    " - **Average linkage** minimizes the average of the distances between all observations of pairs of clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Perform clustering analysis using ward linkage criteria\n",
    "\n",
    "\n",
    "\n",
    "# 'clust_ward' is a numpy array, explore it!\n",
    "\n",
    "\n",
    "# Plot the clustering results using the format_plot function\n",
    "\n",
    "\n",
    "\n",
    "### ----\n",
    "\n",
    "\n",
    "# Perform clustering analysis using complete linkage criteria\n",
    "\n",
    "\n",
    "\n",
    "# 'clust_complete' is a numpy array, explore it!\n",
    "\n",
    "\n",
    "# Plot the clustering results using the format_plot function\n",
    "\n",
    "\n",
    "\n",
    "### ----\n",
    "\n",
    "# Perform clustering analysis using average linkage criteria\n",
    "\n",
    "\n",
    "\n",
    "# 'clust_average' is a numpy array, explore it!\n",
    "\n",
    "\n",
    "# Plot the clustering results using the format_plot function\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ Step 9: Clustering Analysis Using DBSCAN Method\n",
    "\n",
    "ðŸ“‹ DBSCAN - Density-Based Spatial Clustering of Applications with Noise (i.e., outliers). \n",
    "\n",
    "ðŸ’¡ The main idea behind DBSCAN is that a data value belongs to a cluster if it is close to many data values from that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "# Train the dbscan model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate predictions using the dbscan model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot clustering results using the format_plot function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ðŸ“ Note that DBSCAN does not require specifying the number of clusters, let's review it ('db_pred')\n",
    "\n",
    "\n",
    "\n",
    "# Noisy data points are given the label -1. They do not belong to any cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  `PART 2`: Prediction of Production Patterns Using Clustering Analysis\n",
    "\n",
    "In the second part of this tutorial, we will learn how to train a clustering model using the production data from four wells, and then use this model to predict the production trends from another well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 10: Read and display the production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read and display the production data (production_data.csv)\n",
    "\n",
    "production = pd.read_csv(...)\n",
    "\n",
    "\n",
    "# âš ï¸ Make sure that the column 'Prod Date' is treated as date (yyyy-mm-dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set the column 'Prod Date' as index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print the production data to visualize changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Investigate the total number of wells in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 11: Create the test set using the production data from Well w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe that contains only the production data from well 'w1', call it 'production_test'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 12: Create the training set using the production data from Wells w2, w3 and w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe that contains the production data from the rest of the wells ('w2', 'w3' and 'w4'), call it 'production_train'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the wells contained in each dataframe ('production_test' & 'production_train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 13: Plot the production data for the train wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's plot the monthly gas production versus date for the train wells  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘† Note that the production from the three wells begins at different times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸStep 14: Plot the \"combined\" production data from the train wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the entire production data without discretizing it by well \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘† In the plot above, the \"production peaks\" correspond to the order in which the wells appear in the dataset (w3 -> w2 -> w4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ðŸ’¡ Now that we have combined the production data from the train wells, the next step will be to train a model to see if we can separate different parts of the time series using another feature. \n",
    "\n",
    "For this, we can create a new feature using the .diff() function ðŸ‘‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 15: Create a new variable using the .diff() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœðŸ¼ The .diff() function is used to calculate the difference between the values for each row and, by default, the previous row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function to add a new feature to the production dataset, call it 'preprocess_data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the 'preprocess_data' function to the production test and production train dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the resulting production train dataframe 'production_train_processed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the resulting production test dataframe 'production_test_processed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 16: Plot the Production and diff features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the production (Monthly Gas) and diff variables in the same graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 17: Identify the optimum number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use the elbow plot on the train dataset to identify the optimum number of clusters\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 18: Train a K-Means model on the production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train a K-means clustering model using the production data on the train set (w2, w3, w4)\n",
    "\n",
    "\n",
    "\n",
    "# 'prod_train_pred' is a numpy array, explore it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 19: Plot the K-means Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the clustering on the train set (w2, w3, w4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 20: Use the K-means model to predict the Production data for well w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the previously trained K-means model to predict the production trend on the test set (w1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot the results of the clustering on the test set (w1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 21: Plot the Actual Production Data for Well w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now let's plot the actual production data for well 'w1' and compare it with the predicted production clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸš€ Well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš§ Optional steps beyond this point ðŸš§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Step 22: Decline Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the production data from a CSV file\n",
    "\n",
    "\n",
    "# Filter the data for well 1 -> 'w1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns to 'Date', 'UWI', and 'Qg1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Date and Qg1 columns into numpy arrays\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the time range array 'xdataf'\n",
    " \n",
    "\n",
    "# xdataf refers to the range of time (e.g., from 0 to 90 months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ Now let's create a new dataframe combining the required variables for DCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with 'zmonth', 'Qg1', and 'Date' columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¨ ARPS EQUATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/pic7.png\" alt=\"Decline Curve Equation\" style= \"width: 500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# 1) Determine the total number of historical production and create a range from 1 to 90+\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 2) Determine the bounds (min-max)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "# 3) Get the values of months and gas rate\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "# 4) Define the exponential equation with three variables: time, initial rate, and rate of decline\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 5) Fit the exponential decline equation with the rate observations using curve_fit\n",
    "# Determine the values of the initial rate and rate of decline.\n",
    "\n",
    " # Print the values of qi1 and Di1.\n",
    "\n",
    "# Generate the predicted gas rate using the fitted parameters\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "# 6) Plot monthly production and the exponential decline curve with the fitted curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 7) Repeat the same process as above, but now filter from the yellow cluster,  which indicates that the well has reached a boundary-dominated flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fit only with the filtered portion of the gas rate\n",
    "\n",
    "# Get the length of the zmonth column\n",
    "\n",
    "\n",
    "# Update the zmonth column with a range from 1 to n1\n",
    "\n",
    "\n",
    "# Find the maximum gas rate\n",
    "\n",
    "\n",
    "# Set the initial guess for curve fitting\n",
    "\n",
    "\n",
    "# Filter the zmonth and Qg1 columns to exclude the first 15 months\n",
    "\n",
    "\n",
    "# Define the exponential decline function\n",
    "\n",
    "\n",
    "# Perform the curve fitting to estimate qi1 and Di1\n",
    "\n",
    "\n",
    "# Generate the predicted gas production using the fitted parameters\n",
    "\n",
    "\n",
    "\n",
    "# Plot the predicted production and the actual production\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Extract printed initial rate and decline rate values from the previous step\n",
    "\n",
    "\n",
    "\n",
    "# 9) Select the number of days to make the forecast.\n",
    "\n",
    "\n",
    "\n",
    "# 10) Create a data frame with the column 'zmonth,' which includes the total months of history plus the forecasting months\n",
    "\n",
    "\n",
    "\n",
    "# 11) Loop from the initial cluster yellow rate, and apply the exponential equation using the fitted values to predict \n",
    "# the gas rate for the following 100 months.  \n",
    "# Finally, add those predictions to the data frame we created in step 11.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's print the forecasted data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ¯ Well done!"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1663181300495,
  "creator": "ashamsa@slb.com",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "modifiedBy": "dbecerra6@slb.com",
  "tags": [],
  "vscode": {
   "interpreter": {
    "hash": "4818919a9fcbb1ea87930a6929e5cae70da81908a23f72e084a8ab43e96a51f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
