{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c63fa68",
   "metadata": {},
   "source": [
    "<img src=\"../images/logo.png\" alt=\"slb\" style= \"width: 1700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚡️   - Tutorial 1: Predicting Cumulative Production\n",
    "\n",
    "💡 The objective of this exercise is to learn how to apply supervised learning to predict an output variable from multi-dimensional data\n",
    "\n",
    "📋 The goal is to predict production data using completion parameters from ~600 wells targeting an unconventional reservoir. In addition, we have available the location (lat, long) and total depth (tvd) of the wells, which can be used as a geology proxy. \n",
    "\n",
    "👌 Once we have completed our prediction, we will also investigate which feature (variable) has the most contribution on the well production. For this purpose we will use `SHAP` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 1: Data Load and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read the well dataset (well_data_raw.csv) and store it in a dataframe named 'well_data'\n",
    "\n",
    "well_data= pd.read_csv(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Explore the content of the well_data dataframe\n",
    "\n",
    "well_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot to display the well location (lat & long), total depth (tvd), and the cumulative gas production\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Display and analyze the distribution of missing values in the dataset\n",
    "\n",
    "import missingno as msno\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍️ `missingno` is a nice library to visualize the distribution of null values. \n",
    "\n",
    "Note that visualizing null values can also reveal some relationships between the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another plot to visualize the total number of missing values per column is the bar plot. Let's display it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧹 Now let's do some data clean up 🧹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First, create a copy of the dataset (well_data), call it 'well_data_clean' and perform the following operations:\n",
    "\n",
    "# 1) Make a copy of the original dataset (to be safe! 🤓)\n",
    "# 2) Set the column 'well_id' as the index \n",
    "# 3) Remove the rows where 'fluid'= 'oil'\n",
    "# 4) Remove missing values\n",
    "# 5) Remove the column 'fluid'\n",
    "# 6) Add a new column 'target_bin' and define 5 bins for the 'cum_gas_prod' -> [\"very_low\", \"low\", \"mid\", \"high\", \"very_high\"]\n",
    "# 7) Assign the data type in the 'target_bin' column as object\n",
    "\n",
    "\n",
    "\n",
    "# Let's print the new dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Do you know the meaning of the tilde (~) operator in python? We just used it in the step above 👆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> 💡 Hint </summary>\n",
    "    \n",
    "👌 In Python, the tilde (~) operator is a bitwise NOT operator, which means it performs a logical NOT operation \n",
    "\n",
    "In the code above, the tilde is being used as a logical NOT operator to invert the boolean mask returned by the str.contains() method\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the count of values in each bin (\"target_bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁Step 2: Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First create a subset of the 'well_data_clean' dataframe to include only the numerical data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the dataset and add back the non-numerical column -> 'target_bin'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Display the standardized dataset 'well_data_clean'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 3: Reshape the DataFrame for Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 To be able to display all the columns in the same plot, we need to do some sort of reshaping on our dataframe.\n",
    "\n",
    "In the cell below we will use the `.melt()` function to create the following shape:\n",
    "\n",
    "**index** ------   **target_bin** ------   **variable** ------  **value**\n",
    "\n",
    "✍️ The .melt() function is  used to create a specific format of the DataFrame where one or more columns work as identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Change the DataFrame format from wide to long using the .melt () function \n",
    "\n",
    "well_data_melt = pd.melt(well_data_clean, value_vars= list(well_data_clean.columns).remove(\"target_bin\"), id_vars= 'target_bin')\n",
    "\n",
    "\n",
    "# Visualize the melted dataframe. Take your time to analyze the content of rows and columns 🧐\n",
    "\n",
    "well_data_melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👌 Having the dataframe in this format will facilitate the plotting in the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁Step 4: Statistical Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a violin plot to visualize the distribution of all variables in the dataset ('well_data_melt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a swarmplot and color the points according to the 'target_bin' variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁Step 5:  Defining the Variables to be Used as 'Target' and 'Features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out the list column names for the well_data_clean dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the list of variables to be used as features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the cumulative gas production as the target variable 'cum_12_gas_prod', call it 'target'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 6: Split the Dataset into 'train set' and 'test set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the full dataset ('well_data') into two parts [30: 70] [test:train]\n",
    "\n",
    "\n",
    "\n",
    "# ✍️ 'X' refers to the features, and 'y' refer to the target\n",
    "\n",
    "# ✍️ 'test_size= 0.3' represents the proportion of data points that will be in the test set (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 7: Train a LightGBM Model and Evaluate its Performance using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 We are going to try a powerful tree-based method -> `lightgbm`\n",
    "\n",
    "📋 Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample\n",
    "\n",
    "📌 In cross  validation, k refers to the number of groups that a given data sample is to be split into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a cross-validation strategy. Define the number of folds (k=5) to split the data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the classification model using a LightGBM Classifier and visualizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the data to the visualizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝ The plot above shows the performance of the LGBM model on 5 iterations. \n",
    "\n",
    "Since we defined *k* = 5, for each run, 20% of the data is set aside for testing. Once the model is trained, its score is shown as a bar in the chart.\n",
    "\n",
    "Usually we apply this method on the entire data. But we can also check the performance on the test data 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 8: Use *yellowbrick* to Visualize the Performance of the LightGBM Model on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# Generate a prediction error plot to evaluate the LGBM model\n",
    "\n",
    "'LGMB : Light Gradient Boosting Machine'\n",
    "\n",
    "\n",
    "'y = Actual Target Value'\n",
    "'ŷ = Predicted Target value'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 9: Train and Evaluate a XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Now let's try another *hackaton-wining* model -> `XGBRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Generate the prediction error plot for the XGBoost model\n",
    "\n",
    "'XGBoost: Extreme Gradient Boosting'\n",
    "\n",
    "\n",
    "# 'y = Actual Target Value'\n",
    "# 'ŷ = Predicted Target value'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 10: Generate a Residuals Plot\n",
    "\n",
    "✍️ Residuals, in the context of regression models, are the difference between the observed value of the target variable (y) and the predicted value (ŷ). In other words, the error of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "# Now let's create a residual plot for the LGBM model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝ We do not want to see any trend in the residual plot. The randomness in the residuals seems to indicate that our model is performing well. \n",
    "\n",
    "👌 We can also see from the histogram on the right that our error is normally distributed around zero, which also generally indicates a well fitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 11: Plot the Learning Curve for the LGBM Model\n",
    "\n",
    "\n",
    "💡 A learning curve shows the relationship of the training score versus the cross validated test score for an estimator with a varying number of training samples\n",
    "\n",
    "⚠️ If the training score is much greater than the validation score, then the model probably requires more training data in order to generalize more effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# Create an evenly spaced array that will be used as training instances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the learning curve visualizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the learning curve to our well data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💥 The training score measures how well the model fits the training data, while the cross-validation score measures how well the model generalizes to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 The learning curve above suggests that providing more data for training improves the model performance (score)\n",
    "\n",
    "👉 The separation between training and CV curves in the learning curve plot indicates that the model is overfitting the training data. It means that the model is fitting the training data too closely and not generalizing well to new data.\n",
    "    \n",
    "<br>\n",
    "📋 To improve the model we can:  \n",
    "\n",
    "1- Provide more data for training\n",
    "\n",
    "2- Improve our feature engineering\n",
    "\n",
    "3- Try using a different model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 12: Feature Importance\n",
    "\n",
    "💡 The feature engineering process involves selecting manipulating, and transforming raw data into features that can be used to produce a valid model. \n",
    "\n",
    "📋 Generally, a model that has less features is preferred even if the score is slightly lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection.importances import FeatureImportances\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Title case the features for a better display\n",
    "\n",
    "\n",
    "\n",
    "# Define the visualizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit and show the feature importance plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Step 13: Use SHAP Values to Explain How a ML Model Works\n",
    "\n",
    "**SHAP**: Shapley Additive Explanations\n",
    "\n",
    "💡 SHAP values quantify the contribution of each feature to the final prediction of the model\n",
    "\n",
    "🔖 Positive SHAP values indicate a positive impact on the prediction, which means that increasing its value will result in an increase in the predicted value of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Train a XGBoost model\n",
    "\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a beeswarm plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👆 From the plot above: \n",
    "\n",
    "🔑 Note that features are also ordered by their effect on prediction\n",
    "\n",
    "🔑 Each point represents a row from the dataset\n",
    "\n",
    "🔑 The colors represent the feature values, not to be confused with the shap values. If the value of a feature is high -> pink\n",
    "\n",
    "🔑 In SHAP, the most important feature is the one that has the highest mean absolute SHAP value across all samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First, select a random well to investigate. For example the well in row 150\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also print to the raw data (before normalization) for the well above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Waterfall plot for the well in row #3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👁‍🗨 The waterfall plot shows the effect that each feature has on the prediction for a given observation, in this case, a well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 If the arrow points to the left, it means that the feature value has a negative impact on the model's prediction. If it points to the right, it means that the feature value has a positive impact on the model's prediction\n",
    "\n",
    "📌 The length of the bar for each feature represents the magnitude of the impact that feature has on the prediction\n",
    "\n",
    "📌 The color of the bar represents the feature value for that particular data point, with red indicating high feature values and blue indicating low feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎯 Well done!"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1663181947742,
  "creator": "ashamsa@slb.com",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "modifiedBy": "dbecerra6@slb.com",
  "tags": [],
  "versionNumber": 1,
  "vscode": {
   "interpreter": {
    "hash": "4818919a9fcbb1ea87930a6929e5cae70da81908a23f72e084a8ab43e96a51f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
